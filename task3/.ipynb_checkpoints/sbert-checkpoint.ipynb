{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d28c4ae3-4743-4b82-a6e9-bc168a4d65f1",
    "deepnote_cell_height": 190.375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# SBERT\n",
    "\n",
    "Implementation of the following papers:\n",
    "\n",
    "BERT: https://arxiv.org/pdf/1810.04805.pdf\n",
    "SBERT: https://arxiv.org/pdf/1908.10084v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "b6b1eed8-de1e-48ce-81df-90e68af174d5",
    "deepnote_cell_height": 937.578125,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     597.1875
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 916,
    "execution_start": 1646903595573,
    "source_hash": "f8ac9266",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from preprocess import Preprocess\n",
    "import torch\n",
    "import numpy as np\n",
    "import sts_data\n",
    "from sts_data import STSData\n",
    "from importlib import reload\n",
    "from transformers import BertModel\n",
    "from train import train_model\n",
    "from evaluation import evaluate_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:loading and preprocessing data...\n",
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Reusing dataset sick (C:\\Users\\Turtuk PC\\.cache\\huggingface\\datasets\\sick\\default\\0.0.0\\c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db)\n",
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Reusing dataset sick (C:\\Users\\Turtuk PC\\.cache\\huggingface\\datasets\\sick\\default\\0.0.0\\c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db)\n",
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Reusing dataset sick (C:\\Users\\Turtuk PC\\.cache\\huggingface\\datasets\\sick\\default\\0.0.0\\c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db)\n",
      "INFO:root:reading and preprocessing data completed...\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function STSData.get_data_loader.<locals>.<lambda> at 0x000001F06D6D0F28> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1a3b3397974e2fb4c1ea217e0049bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8232c3fdc7844190812874f40c73c060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00446f9808a84004808bd32531398044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160e37b1ddb3458786d0546179c083ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:creating dataloaders completed...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stopwords_path=\"stopwords-en.txt\"\n",
    "dataset_name = \"sick\"\n",
    "batch_size = 64\n",
    "\n",
    "reload(sts_data)\n",
    "\n",
    "dataset_name = \"sick\"\n",
    "\n",
    "sbert_data = STSData(\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "## fetching dataloaders\n",
    "sbert_dataloaders = sbert_data.get_data_loader(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [19:06<00:00, 16.38s/it]\n",
      "INFO:root:Accuracy: (0.7346289164275696, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# importing pre-trained BERT base model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# setting Adam optimizer with learning rate as specified by SBERT paper\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "max_epochs = 1\n",
    "\n",
    "train_acc, train_loss = train_model(model, optimizer, sbert_dataloaders['train'], max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "7c4b1095-4617-4d80-af82-41b29aaf87e5",
    "deepnote_cell_height": 598,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 51,
    "execution_start": 1646902846369,
    "scrolled": true,
    "source_hash": "cbd665ab",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019411560547138964\n"
     ]
    }
   ],
   "source": [
    "print(train_loss/len(sbert_dataloaders['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './sbert_fine_tuned'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 77/77 [14:10<00:00, 11.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7770470357342115 \n"
     ]
    }
   ],
   "source": [
    "test_model = BertModel.from_pretrained(model_path)\n",
    "\n",
    "evaluate_test_set(test_model,sbert_dataloaders['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "1. \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\", Nils Reimers and Iryna Gurevych\n",
    "2. \"Training Sentence Transformers the OG Way (with Softmax Loss)\", Pinecone\n",
    "    https://www.pinecone.io/learn/train-sentence-transformers-softmax/\n",
    "3. https://medium.com/swlh/transformer-based-sentence-embeddings-cd0935b3b1e0\n",
    "\n",
    "\n",
    "Library used for pre-trained model:\n",
    "https://www.sbert.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "28b6770b-6743-4128-a14b-457560d36e72",
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
